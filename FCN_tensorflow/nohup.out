Step: 0, Train_loss:91.932
2018-10-13 22:56:30.057739 ---> Validation_loss: 81.5896
Step: 100, Train_loss:3.90822
Step: 200, Train_loss:1.86005
Step: 300, Train_loss:0.341247
Step: 400, Train_loss:0.213466
Step: 500, Train_loss:0.154835
Step: 600, Train_loss:0.230651
Step: 700, Train_loss:0.312147
Step: 800, Train_loss:0.167779
Step: 900, Train_loss:0.257909
Step: 1000, Train_loss:0.139115
Step: 1100, Train_loss:0.207696
Step: 1200, Train_loss:0.207748
Step: 1300, Train_loss:0.23799
Step: 1400, Train_loss:0.217586
Step: 1500, Train_loss:0.222731
Step: 1600, Train_loss:0.194837
Step: 1700, Train_loss:0.237909
Step: 1800, Train_loss:0.151026
Step: 1900, Train_loss:0.1108
Step: 2000, Train_loss:0.131746
Step: 2100, Train_loss:0.282846
Step: 2200, Train_loss:0.158422
Step: 2300, Train_loss:0.184192
Step: 2400, Train_loss:0.202524
Step: 2500, Train_loss:0.122424
Step: 2600, Train_loss:0.265068
Step: 2700, Train_loss:0.118852
Step: 2800, Train_loss:0.103792
Step: 2900, Train_loss:0.191695
Step: 3000, Train_loss:0.117796
2018-10-13 23:11:10.211806 ---> Validation_loss: 0.174098
Step: 3100, Train_loss:0.103562
Step: 3200, Train_loss:0.148723
Step: 3300, Train_loss:0.127424
Step: 3400, Train_loss:0.0746611
Step: 3500, Train_loss:0.0980195
Step: 3600, Train_loss:0.0957088
Step: 3700, Train_loss:0.176291
Step: 3800, Train_loss:0.121022
Step: 3900, Train_loss:0.118037
Step: 4000, Train_loss:0.116056
Step: 4100, Train_loss:0.134819
Step: 4200, Train_loss:0.0936813
Step: 4300, Train_loss:0.083408
Step: 4400, Train_loss:0.10782
Step: 4500, Train_loss:0.0889501
Step: 4600, Train_loss:0.150673
Step: 4700, Train_loss:0.101773
Step: 4800, Train_loss:0.103435
Step: 4900, Train_loss:0.103319
Step: 5000, Train_loss:0.0969865
Step: 5100, Train_loss:0.0892106
Step: 5200, Train_loss:0.12996
Step: 5300, Train_loss:0.105079
Step: 5400, Train_loss:0.136891
Step: 5500, Train_loss:0.0975522
Step: 5600, Train_loss:0.176987
Step: 5700, Train_loss:0.0669042
Step: 5800, Train_loss:0.122569
Step: 5900, Train_loss:0.0832616
Step: 6000, Train_loss:0.0831753
2018-10-13 23:25:49.397994 ---> Validation_loss: 0.14883
Step: 6100, Train_loss:0.134463
Step: 6200, Train_loss:0.129492
Step: 6300, Train_loss:0.106657
Step: 6400, Train_loss:0.0783751
Step: 6500, Train_loss:0.0969076
Step: 6600, Train_loss:0.13979
Step: 6700, Train_loss:0.0722528
Step: 6800, Train_loss:0.100184
w Step: 6900, Train_loss:0.0849878
Step: 7000, Train_loss:0.0658168
Step: 7100, Train_loss:0.0850494
Step: 7200, Train_loss:0.046481
Step: 7300, Train_loss:0.0809432
Step: 7400, Train_loss:0.0557005
Step: 7500, Train_loss:0.106904
Step: 7600, Train_loss:0.0552692
Step: 7700, Train_loss:0.109149
Step: 7800, Train_loss:0.114193
Step: 7900, Train_loss:0.0535859
Step: 8000, Train_loss:0.108872
Step: 8100, Train_loss:0.0916084
Step: 8200, Train_loss:0.0694369
Step: 8300, Train_loss:0.0749698
Step: 8400, Train_loss:0.104112
Step: 8500, Train_loss:0.0577181
Step: 8600, Train_loss:0.059839
Step: 8700, Train_loss:0.0541771
****************** Epochs completed: 10******************
Step: 8800, Train_loss:0.0782415
Step: 8900, Train_loss:0.0884724
Step: 9000, Train_loss:0.0430449
2018-10-13 23:40:28.812092 ---> Validation_loss: 0.153421
Step: 9100, Train_loss:0.0851835
Step: 9200, Train_loss:0.0831237
Step: 9300, Train_loss:0.0749901
Step: 9400, Train_loss:0.0392462
Step: 9500, Train_loss:0.0685488
Step: 9600, Train_loss:0.062296
Step: 9700, Train_loss:0.0596956
Step: 9800, Train_loss:0.0547408
Step: 9900, Train_loss:0.0536499
Step: 10000, Train_loss:0.0730844
Step: 10100, Train_loss:0.0603328
Step: 10200, Train_loss:0.0525113
Step: 10300, Train_loss:0.0780723
Step: 10400, Train_loss:0.0641191
Step: 10500, Train_loss:0.051377
Step: 10600, Train_loss:0.0462514
Step: 10700, Train_loss:0.0556472
Step: 10800, Train_loss:0.0579481
Step: 10900, Train_loss:0.0638156
Step: 11000, Train_loss:0.0341643
Step: 11100, Train_loss:0.0340003
Step: 11200, Train_loss:0.0495704
Step: 11300, Train_loss:0.0870142
Step: 11400, Train_loss:0.0336579
Step: 11500, Train_loss:0.0481591
Step: 11600, Train_loss:0.0431816
Step: 11700, Train_loss:0.0321436
Step: 11800, Train_loss:0.0302733
Step: 11900, Train_loss:0.0344054
Step: 12000, Train_loss:0.0352615
2018-10-13 23:55:08.266491 ---> Validation_loss: 0.084797
Step: 12100, Train_loss:0.0361241
Step: 12200, Train_loss:0.0493884
Step: 12300, Train_loss:0.0946287
Step: 12400, Train_loss:0.0544389
Step: 12500, Train_loss:0.0271538
Step: 12600, Train_loss:0.0486908
Step: 12700, Train_loss:0.052836
Step: 12800, Train_loss:0.0377361
Step: 12900, Train_loss:0.0531408
Step: 13000, Train_loss:0.0328091
Step: 13100, Train_loss:0.0510408
Step: 13200, Train_loss:0.0353557
Step: 13300, Train_loss:0.0761463
Step: 13400, Train_loss:0.051126
Step: 13500, Train_loss:0.0480064
Step: 13600, Train_loss:0.0571754
Step: 13700, Train_loss:0.0740152
Step: 13800, Train_loss:0.0626367
Step: 13900, Train_loss:0.0637172
Step: 14000, Train_loss:0.0400927
Step: 14100, Train_loss:0.0531359
Step: 14200, Train_loss:0.0420428
Step: 14300, Train_loss:0.0427583
Step: 14400, Train_loss:0.0275676
Step: 14500, Train_loss:0.0355396
Step: 14600, Train_loss:0.038546
Step: 14700, Train_loss:0.0516212
Step: 14800, Train_loss:0.0300007
Step: 14900, Train_loss:0.0454011
Step: 15000, Train_loss:0.0274348
2018-10-14 00:09:48.273601 ---> Validation_loss: 0.146974
Step: 15100, Train_loss:0.0533463
Step: 15200, Train_loss:0.0368576
Step: 15300, Train_loss:0.0565161
Step: 15400, Train_loss:0.0481935
Step: 15500, Train_loss:0.0431613
Step: 15600, Train_loss:0.0278769
Step: 15700, Train_loss:0.0466441
Step: 15800, Train_loss:0.0455253
Step: 15900, Train_loss:0.0414204
Step: 16000, Train_loss:0.0255576
Step: 16100, Train_loss:0.0421294
Step: 16200, Train_loss:0.0484436
Step: 16300, Train_loss:0.044258
Step: 16400, Train_loss:0.0364171
Step: 16500, Train_loss:0.0273166
Step: 16600, Train_loss:0.0291306
Step: 16700, Train_loss:0.0288935
Step: 16800, Train_loss:0.0246244
Step: 16900, Train_loss:0.0406911
Step: 17000, Train_loss:0.0415595
Step: 17100, Train_loss:0.025037
Step: 17200, Train_loss:0.0512635
Step: 17300, Train_loss:0.0267494
Step: 17400, Train_loss:0.0297551
Step: 17500, Train_loss:0.0262682
****************** Epochs completed: 20******************
Step: 17600, Train_loss:0.0397295
Step: 17700, Train_loss:0.0393288
Step: 17800, Train_loss:0.0752553
Step: 17900, Train_loss:0.0488176
Step: 18000, Train_loss:0.0404449
2018-10-14 00:24:28.049715 ---> Validation_loss: 0.164781
Step: 18100, Train_loss:0.034035
Step: 18200, Train_loss:0.0442164
Step: 18300, Train_loss:0.0367209
Step: 18400, Train_loss:0.063131
Step: 18500, Train_loss:0.0382797
Step: 18600, Train_loss:0.0219854
Step: 18700, Train_loss:0.0271577
Step: 18800, Train_loss:0.031476
Step: 18900, Train_loss:0.039327
Step: 19000, Train_loss:0.0271702
Step: 19100, Train_loss:0.0407669
Step: 19200, Train_loss:0.0362547
Step: 19300, Train_loss:0.0452248
Step: 19400, Train_loss:0.022924
Step: 19500, Train_loss:0.0429689
Step: 19600, Train_loss:0.0284833
Step: 19700, Train_loss:0.0338089
Step: 19800, Train_loss:0.0258881
Step: 19900, Train_loss:0.0365274
Step: 20000, Train_loss:0.0252588
Step: 20100, Train_loss:0.0384455
Step: 20200, Train_loss:0.0340068
Step: 20300, Train_loss:0.0179734
Step: 20400, Train_loss:0.0365713
Step: 20500, Train_loss:0.0370303
Step: 20600, Train_loss:0.0319659
Step: 20700, Train_loss:0.0272016
Step: 20800, Train_loss:0.0557712
Step: 20900, Train_loss:0.0368001
Step: 21000, Train_loss:0.0245198
2018-10-14 00:39:07.790459 ---> Validation_loss: 0.23517
Step: 21100, Train_loss:0.0475297
Step: 21200, Train_loss:0.02742
Step: 21300, Train_loss:0.0268026
Step: 21400, Train_loss:0.025035
Step: 21500, Train_loss:0.0356188
Step: 21600, Train_loss:0.0430705
Step: 21700, Train_loss:0.0402704
Step: 21800, Train_loss:0.0385004
Step: 21900, Train_loss:0.0258371
Step: 22000, Train_loss:0.0347159
Step: 22100, Train_loss:0.0364651
Step: 22200, Train_loss:0.0224213
Step: 22300, Train_loss:0.0356173
Step: 22400, Train_loss:0.040252
Step: 22500, Train_loss:0.0332749
Step: 22600, Train_loss:0.0280968
Step: 22700, Train_loss:0.0286081
Step: 22800, Train_loss:0.0400832
Step: 22900, Train_loss:0.0287965
Step: 23000, Train_loss:0.0169475
Step: 23100, Train_loss:0.0306997
Step: 23200, Train_loss:0.0251162
Step: 23300, Train_loss:0.0273314
Step: 23400, Train_loss:0.0160568
Step: 23500, Train_loss:0.0247554
Step: 23600, Train_loss:0.0344686
Step: 23700, Train_loss:0.0347667
Step: 23800, Train_loss:0.0364687
Step: 23900, Train_loss:0.0341425
Step: 24000, Train_loss:0.0323094
2018-10-14 00:53:47.034899 ---> Validation_loss: 0.256145
Step: 24100, Train_loss:0.0214136
Step: 24200, Train_loss:0.0322657
Step: 24300, Train_loss:0.0385912
Step: 24400, Train_loss:0.0461776
Step: 24500, Train_loss:0.0297164
Step: 24600, Train_loss:0.0214511
Step: 24700, Train_loss:0.0166661
Step: 24800, Train_loss:0.0454678
Step: 24900, Train_loss:0.0270324
Step: 25000, Train_loss:0.031783
Step: 25100, Train_loss:0.0327823
Step: 25200, Train_loss:0.0249641
Step: 25300, Train_loss:0.0368513
Step: 25400, Train_loss:0.0328102
Step: 25500, Train_loss:0.0264934
Step: 25600, Train_loss:0.0327165
Step: 25700, Train_loss:0.0163236
Step: 25800, Train_loss:0.0229221
Step: 25900, Train_loss:0.0224423
Step: 26000, Train_loss:0.0212432
Step: 26100, Train_loss:0.0187863
Step: 26200, Train_loss:0.0187146
Step: 26300, Train_loss:0.0229659
****************** Epochs completed: 30******************
Step: 26400, Train_loss:0.021249
Step: 26500, Train_loss:0.0167173
Step: 26600, Train_loss:0.0346592
Step: 26700, Train_loss:0.0233933
Step: 26800, Train_loss:0.0284233
Step: 26900, Train_loss:0.0164477
Step: 27000, Train_loss:0.023007
2018-10-14 01:08:26.028966 ---> Validation_loss: 0.407234
Step: 27100, Train_loss:0.0195615
Step: 27200, Train_loss:0.0221002
Step: 27300, Train_loss:0.0186795
Step: 27400, Train_loss:0.0273902
Step: 27500, Train_loss:0.0233102
Step: 27600, Train_loss:0.0293546
Step: 27700, Train_loss:0.0163763
Step: 27800, Train_loss:0.0139314
Step: 27900, Train_loss:0.0281269
Step: 28000, Train_loss:0.0185774
Step: 28100, Train_loss:0.0226909
Step: 28200, Train_loss:0.0191262
Step: 28300, Train_loss:0.0165736
Step: 28400, Train_loss:0.0139188
Step: 28500, Train_loss:0.0211549
Step: 28600, Train_loss:0.0214944
Step: 28700, Train_loss:0.0253902
Step: 28800, Train_loss:0.0178163
Step: 28900, Train_loss:0.0207958
Step: 29000, Train_loss:0.0312105
Step: 29100, Train_loss:0.0315018
Step: 29200, Train_loss:0.0179845
Step: 29300, Train_loss:0.0226531
Step: 29400, Train_loss:0.0226688
Step: 29500, Train_loss:0.0253997
Step: 29600, Train_loss:0.0321086
Step: 29700, Train_loss:0.0142214
Step: 29800, Train_loss:0.0155643
Step: 29900, Train_loss:0.017646
Step: 30000, Train_loss:0.0290825
2018-10-14 01:23:04.637632 ---> Validation_loss: 0.46139
****************** Epochs completed: 10******************
Step: 30100, Train_loss:0.0208793
Step: 30200, Train_loss:0.0301719
Step: 30300, Train_loss:0.0179705
Step: 30400, Train_loss:0.0255571
Step: 30500, Train_loss:0.023495
Step: 30600, Train_loss:0.0218434
Step: 30700, Train_loss:0.0210654
Step: 30800, Train_loss:0.0338045
Step: 30900, Train_loss:0.0253935
Step: 31000, Train_loss:0.025944
Step: 31100, Train_loss:0.0350595
Step: 31200, Train_loss:0.0148255
Step: 31300, Train_loss:0.0148963
Step: 31400, Train_loss:0.0325687
Step: 31500, Train_loss:0.0108033
Step: 31600, Train_loss:0.024713
Step: 31700, Train_loss:0.0307387
Step: 31800, Train_loss:0.0279639
Step: 31900, Train_loss:0.0137829
Step: 32000, Train_loss:0.0252268
Step: 32100, Train_loss:0.0237659
Step: 32200, Train_loss:0.016481
Step: 32300, Train_loss:0.023992
Step: 32400, Train_loss:0.0224495
Step: 32500, Train_loss:0.032015
Step: 32600, Train_loss:0.011763
Step: 32700, Train_loss:0.015469
Step: 32800, Train_loss:0.0111261
Step: 32900, Train_loss:0.0275466
Step: 33000, Train_loss:0.0266086
2018-10-14 01:37:43.151041 ---> Validation_loss: 0.430579
Step: 33100, Train_loss:0.018878
Step: 33200, Train_loss:0.0219318
Step: 33300, Train_loss:0.0227318
Step: 33400, Train_loss:0.0198589
Step: 33500, Train_loss:0.0166418
Step: 33600, Train_loss:0.0181496
Step: 33700, Train_loss:0.0193002
Step: 33800, Train_loss:0.0194112
Step: 33900, Train_loss:0.0155209
Step: 34000, Train_loss:0.0194986
Step: 34100, Train_loss:0.0292776
Step: 34200, Train_loss:0.0233001
Step: 34300, Train_loss:0.0245383
Step: 34400, Train_loss:0.0175304
Step: 34500, Train_loss:0.0226634
Step: 34600, Train_loss:0.0224194
Step: 34700, Train_loss:0.0222222
Step: 34800, Train_loss:0.0224793
Step: 34900, Train_loss:0.0178759
Step: 35000, Train_loss:0.0229044
Step: 35100, Train_loss:0.0266075
****************** Epochs completed: 40******************
Step: 35200, Train_loss:0.0267521
Step: 35300, Train_loss:0.0157992
Step: 35400, Train_loss:0.0247573
Step: 35500, Train_loss:0.0195642
Step: 35600, Train_loss:0.0259634
Step: 35700, Train_loss:0.0126299
Step: 35800, Train_loss:0.0196235
Step: 35900, Train_loss:0.0140232
Step: 36000, Train_loss:0.0196856
2018-10-14 01:52:22.032741 ---> Validation_loss: 0.485917
Step: 36100, Train_loss:0.0271891
Step: 36200, Train_loss:0.024336
Step: 36300, Train_loss:0.0173886
Step: 36400, Train_loss:0.0214453
Step: 36500, Train_loss:0.0269344
Step: 36600, Train_loss:0.0223467
Step: 36700, Train_loss:0.0122938
Step: 36800, Train_loss:0.0154183
Step: 36900, Train_loss:0.0143818
Step: 37000, Train_loss:0.0149399
Step: 37100, Train_loss:0.0212245
Step: 37200, Train_loss:0.0197257
Step: 37300, Train_loss:0.023095
Step: 37400, Train_loss:0.0135765
Step: 37500, Train_loss:0.0158534
Step: 37600, Train_loss:0.0168053
Step: 37700, Train_loss:0.0272488
Step: 37800, Train_loss:0.0193418
Step: 37900, Train_loss:0.0232694
Step: 38000, Train_loss:0.0130231
Step: 38100, Train_loss:0.0121931
Step: 38200, Train_loss:0.0189238
Step: 38300, Train_loss:0.0131698
Step: 38400, Train_loss:0.0260358
Step: 38500, Train_loss:0.0141681
Step: 38600, Train_loss:0.0195103
Step: 38700, Train_loss:0.0187162
Step: 38800, Train_loss:0.0288001
Step: 38900, Train_loss:0.0208008
Step: 39000, Train_loss:0.0109654
2018-10-14 02:07:01.093882 ---> Validation_loss: 0.551927
Step: 39100, Train_loss:0.0142477
Step: 39200, Train_loss:0.0245088
Step: 39300, Train_loss:0.0229951
Step: 39400, Train_loss:0.0201324
Step: 39500, Train_loss:0.0157438
Step: 39600, Train_loss:0.0147699
Step: 39700, Train_loss:0.0194477
Step: 39800, Train_loss:0.0207431
Step: 39900, Train_loss:0.0230452
Step: 40000, Train_loss:0.0210101
Step: 40100, Train_loss:0.013286
Step: 40200, Train_loss:0.0216899
Step: 40300, Train_loss:0.019738
Step: 40400, Train_loss:0.0113404
Step: 40500, Train_loss:0.013427
Step: 40600, Train_loss:0.0200987
Step: 40700, Train_loss:0.0135514
Step: 40800, Train_loss:0.0205217
Step: 40900, Train_loss:0.0136252
Step: 41000, Train_loss:0.0219119
Step: 41100, Train_loss:0.0189987
Step: 41200, Train_loss:0.0132971
Step: 41300, Train_loss:0.0136197
Step: 41400, Train_loss:0.0196924
Step: 41500, Train_loss:0.0213732
Step: 41600, Train_loss:0.0204828
Step: 41700, Train_loss:0.0115544
Step: 41800, Train_loss:0.0219901
Step: 41900, Train_loss:0.0171895
Step: 42000, Train_loss:0.0193626
2018-10-14 02:21:39.996325 ---> Validation_loss: 0.572124
Step: 42100, Train_loss:0.0204259
Step: 42200, Train_loss:0.0190713
Step: 42300, Train_loss:0.0191355
Step: 42400, Train_loss:0.0123021
Step: 42500, Train_loss:0.0249869
Step: 42600, Train_loss:0.0239489
Step: 42700, Train_loss:0.0147389
Step: 42800, Train_loss:0.0202967
Step: 42900, Train_loss:0.0168593
Step: 43000, Train_loss:0.0123358
Step: 43100, Train_loss:0.0197375
Step: 43200, Train_loss:0.015802
Step: 43300, Train_loss:0.0125564
Step: 43400, Train_loss:0.0129396
Step: 43500, Train_loss:0.0191078
Step: 43600, Train_loss:0.0122635
Step: 43700, Train_loss:0.0217726
Step: 43800, Train_loss:0.0129152
Step: 43900, Train_loss:0.0131265
****************** Epochs completed: 50******************
Step: 44000, Train_loss:0.0134193
Step: 44100, Train_loss:0.0263342
Step: 44200, Train_loss:0.0119018
Step: 44300, Train_loss:0.0120002
Step: 44400, Train_loss:0.0170769
Step: 44500, Train_loss:0.0153336
Step: 44600, Train_loss:0.0191173
Step: 44700, Train_loss:0.0136496
Step: 44800, Train_loss:0.0175137
Step: 44900, Train_loss:0.0192127
Step: 45000, Train_loss:0.0170929
2018-10-14 02:36:18.461905 ---> Validation_loss: 0.465282
Step: 45100, Train_loss:0.0114433
Step: 45200, Train_loss:0.012596
Step: 45300, Train_loss:0.0123704
Step: 45400, Train_loss:0.0222904
Step: 45500, Train_loss:0.0208839
Step: 45600, Train_loss:0.0209673
Step: 45700, Train_loss:0.0281225
Step: 45800, Train_loss:0.0103731
Step: 45900, Train_loss:0.0120284
Step: 46000, Train_loss:0.0190718
Step: 46100, Train_loss:0.0119163
Step: 46200, Train_loss:0.0166121
Step: 46300, Train_loss:0.0126999
Step: 46400, Train_loss:0.0196148
Step: 46500, Train_loss:0.0124006
Step: 46600, Train_loss:0.0246147
Step: 46700, Train_loss:0.0125947
Step: 46800, Train_loss:0.0154022
Step: 46900, Train_loss:0.016767
Step: 47000, Train_loss:0.0211315
Step: 47100, Train_loss:0.0189673
Step: 47200, Train_loss:0.0218974
Step: 47300, Train_loss:0.0162869
Step: 47400, Train_loss:0.0112317
Step: 47500, Train_loss:0.01152
Step: 47600, Train_loss:0.015944
Step: 47700, Train_loss:0.0139112
Step: 47800, Train_loss:0.0129429
Step: 47900, Train_loss:0.0179805
Step: 48000, Train_loss:0.0105151
2018-10-14 02:50:57.418902 ---> Validation_loss: 0.486011
Step: 48100, Train_loss:0.0197308
Step: 48200, Train_loss:0.0129873
Step: 48300, Train_loss:0.0152198
Step: 48400, Train_loss:0.0138517
Step: 48500, Train_loss:0.0254038
Step: 48600, Train_loss:0.0150691
Step: 48700, Train_loss:0.0141554
Step: 48800, Train_loss:0.0116084
Step: 48900, Train_loss:0.0137005
Step: 49000, Train_loss:0.0145539
Step: 49100, Train_loss:0.0169349
Step: 49200, Train_loss:0.0129411
Step: 49300, Train_loss:0.0157482
Step: 49400, Train_loss:0.0185251
Step: 49500, Train_loss:0.00810191
Step: 49600, Train_loss:0.0125393
Step: 49700, Train_loss:0.0114929
Step: 49800, Train_loss:0.0169974
Step: 49900, Train_loss:0.0126779
Step: 50000, Train_loss:0.0169557
Step: 50100, Train_loss:0.0190196
Step: 50200, Train_loss:0.0176308
Step: 50300, Train_loss:0.0184215
Step: 50400, Train_loss:0.0162611
Step: 50500, Train_loss:0.0138108
Step: 50600, Train_loss:0.0230829
Step: 50700, Train_loss:0.0196424
Step: 50800, Train_loss:0.0146909
Step: 50900, Train_loss:0.00814091
Step: 51000, Train_loss:0.0125801
2018-10-14 03:05:35.245939 ---> Validation_loss: 0.485208
Step: 51100, Train_loss:0.0132593
Step: 51200, Train_loss:0.0142223
Step: 51300, Train_loss:0.0139193
Step: 51400, Train_loss:0.011342
Step: 51500, Train_loss:0.0169015
Step: 51600, Train_loss:0.0110253
Step: 51700, Train_loss:0.0162481
Step: 51800, Train_loss:0.0189394
Step: 51900, Train_loss:0.0112282
Step: 52000, Train_loss:0.0171502
Step: 52100, Train_loss:0.0166372
Step: 52200, Train_loss:0.0118254
Step: 52300, Train_loss:0.0125428
Step: 52400, Train_loss:0.0191372
Step: 52500, Train_loss:0.011363
Step: 52600, Train_loss:0.0106313
Step: 52700, Train_loss:0.0173354
****************** Epochs completed: 60******************
Step: 52800, Train_loss:0.010891
Step: 52900, Train_loss:0.0111612
Step: 53000, Train_loss:0.0158199
Step: 53100, Train_loss:0.0103661
Step: 53200, Train_loss:0.0170461
Step: 53300, Train_loss:0.00928317
Step: 53400, Train_loss:0.0130914
Step: 53500, Train_loss:0.0114832
Step: 53600, Train_loss:0.0148947
Step: 53700, Train_loss:0.0108316
Step: 53800, Train_loss:0.0191411
Step: 53900, Train_loss:0.00850186
Step: 54000, Train_loss:0.0107357
2018-10-14 03:20:14.016765 ---> Validation_loss: 0.465096
Step: 54100, Train_loss:0.0133306
Step: 54200, Train_loss:0.010208
Step: 54300, Train_loss:0.0139502
Step: 54400, Train_loss:0.0172586
Step: 54500, Train_loss:0.0195862
Step: 54600, Train_loss:0.0150864
Step: 54700, Train_loss:0.0154401
Step: 54800, Train_loss:0.0122333
Step: 54900, Train_loss:0.0234797
Step: 55000, Train_loss:0.0178594
Step: 55100, Train_loss:0.0115628
Step: 55200, Train_loss:0.0122799
Step: 55300, Train_loss:0.0155637
Step: 55400, Train_loss:0.0162926
Step: 55500, Train_loss:0.017413
Step: 55600, Train_loss:0.0171109
Step: 55700, Train_loss:0.011898
Step: 55800, Train_loss:0.00714118
Step: 55900, Train_loss:0.0142103
Step: 56000, Train_loss:0.0137083
Step: 56100, Train_loss:0.0174401
Step: 56200, Train_loss:0.0185499
Step: 56300, Train_loss:0.019869
Step: 56400, Train_loss:0.0112421
Step: 56500, Train_loss:0.0175148
Step: 56600, Train_loss:0.0125599
Step: 56700, Train_loss:0.0133454
Step: 56800, Train_loss:0.0147864
Step: 56900, Train_loss:0.0152803
Step: 57000, Train_loss:0.0119169
2018-10-14 03:34:53.070836 ---> Validation_loss: 0.547082
Step: 57100, Train_loss:0.0157353
Step: 57200, Train_loss:0.0103424
Step: 57300, Train_loss:0.0150515
Step: 57400, Train_loss:0.0133872
Step: 57500, Train_loss:0.0190132
Step: 57600, Train_loss:0.0114022
Step: 57700, Train_loss:0.0176334
Step: 57800, Train_loss:0.0246616
Step: 57900, Train_loss:0.0137135
Step: 58000, Train_loss:0.00956386
Step: 58100, Train_loss:0.0160365
Step: 58200, Train_loss:0.00868808
Step: 58300, Train_loss:0.0153378
Step: 58400, Train_loss:0.018079
Step: 58500, Train_loss:0.0165668
Step: 58600, Train_loss:0.0122217
Step: 58700, Train_loss:0.0187271
Step: 58800, Train_loss:0.019268
Step: 58900, Train_loss:0.0103663
Step: 59000, Train_loss:0.0164737
Step: 59100, Train_loss:0.0124253
Step: 59200, Train_loss:0.0150158
Step: 59300, Train_loss:0.0141272
Step: 59400, Train_loss:0.012238
Step: 59500, Train_loss:0.0128568
Step: 59600, Train_loss:0.0178953
Step: 59700, Train_loss:0.0148733
Step: 59800, Train_loss:0.0130267
Step: 59900, Train_loss:0.013082
Step: 60000, Train_loss:0.0125685
2018-10-14 03:49:32.233684 ---> Validation_loss: 0.423503
